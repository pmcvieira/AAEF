{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color:#F5F5F5;\" width=\"100%\">\n",
    "<tr><td style=\"background-color:#F5F5F5;\"><img src=\"../images/logo.png\" width=\"150\" align='right'/></td></tr>     <tr><td>\n",
    "            <h2><center>Aprendizagem Automática em Engenharia Biomédica</center></h2>\n",
    "            <h3><center>1st Semester - 2025/2026</center></h3>\n",
    "            <h4><center>Universidade Nova de Lisboa - Faculdade de Ciências e Tecnologia</center></h4>\n",
    "</td></tr>\n",
    "    <tr><td><h2><b><center>Lab 2 - Data Preparation</center></b></h2>\n",
    "    <h4><i><b><center>Loading, Visualizing, Describing, Split and Encoding</center></b></i></h4>\n",
    "    <h5><b>Version Control:</b></h5>\n",
    "    <h5>Created by: Hugo Gamboa to AAEB</h5>\n",
    "    <h5>Modified by: Pedro Vieira to AAEF (25/26)</h5>\n",
    "</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Data Preparation for Machine Learning\n",
    "Data preparation is a crucial step in the machine learning pipeline. Raw data often contains inconsistencies, missing values, and outliers, which can negatively impact the performance of machine learning models. To build robust and reliable models, it's essential to clean and transform the data so that it is suitable for analysis.\n",
    "\n",
    "In today's class, we will discuss the importance of data preparation and introduce pandas, a powerful Python library used for data manipulation and analysis. We will also cover the common steps involved in preparing data for machine learning, such as handling missing values, normalizing data, and encoding categorical variables.\n",
    "\n",
    "### 1.1 Why is Data Preparation Important?\n",
    "Machine learning algorithms rely on high-quality data to make accurate predictions. __If the data is noisy, incomplete, or poorly formatted, the model's performance can degrade significantly__. Some reasons why data preparation is important include:\n",
    "\n",
    "* __Improving Model Accuracy__: Clean and well-structured data allows machine learning models to generalize better and make more accurate predictions.\n",
    "* __Handling Missing or Inconsistent Data__: Missing data and inconsistencies can skew results and lead to poor model performance. Proper handling ensures data integrity.\n",
    "* __Feature Engineering__: Creating new features or transforming existing ones helps the model better understand the underlying patterns in the data.\n",
    "* __Reducing Model Complexity__: Removing irrelevant or redundant data simplifies the model, reducing the risk of overfitting and improving interpretability.\n",
    "\n",
    "### 1.2 Pandas: A Python Module for Data Analysis and Data Manipulation \n",
    "Pandas is an open-source Python library that provides fast, flexible, and expressive data structures designed to make data manipulation and analysis easy. It is widely used in data science and machine learning due to its rich set of features for handling data, such as:\n",
    "\n",
    "* __Data Reading and Writing__: Reading and writing data from various file formats (CSV, Excel, SQL, etc.)\n",
    "* __Data Cleaning__: Handling missing data, outliers, duplicates, etc.\n",
    "* __Data Transformation__: Applying mathematical functions, reshaping data, etc.\n",
    "* __Data visualization__: Integrating well with libraries like Matplotlib and Seaborn for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting Started with Pandas\n",
    "\n",
    "### 2.1 Installing and Importing Pandas\n",
    "Before we can use pandas, we need to make sure it is installed. Depending on your setup, there are two common ways to install it: using __pip__ or __Anaconda__.\n",
    "\n",
    "_Note_: __This step is only needed if you havent installed pandas yet!__\n",
    "\n",
    "__Installing with pip__\n",
    "\n",
    "If you are working in a standard Python environment (e.g., in a Jupyter notebook or directly from Python), you can install pandas using __pip__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ONLY NEED TO RUN THIS LINE OF CODE IF YOU HAVEN'T INSTALLED PANDAS YET\n",
    "# you can uncomment the line below and run it if you want to install pandas using pip directly within the notebook\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Installing with Anaconda__\n",
    "\n",
    "If you are using __Anaconda__, you will need to run the following command in the __Conda Prompt__ (__not in the Jupyter notebook__):\n",
    "\n",
    "1. Open the __Anaconda Prompt__ from your start menu.\n",
    "2. Run the following command:\n",
    "\n",
    "_conda install pandas_\n",
    "\n",
    "Alternatively, you can use __Anaconda Navigator to install pandas via the graphical interface__.\n",
    "\n",
    "Once pandas is installed (either with pip or conda), you can import it into your notebook using:\n",
    "(We will also import some other packages that we will need for today. If you haven't installed these packaes yet you can use the same approach described above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # data science\n",
    "import numpy as np # mathematics\n",
    "import matplotlib.pyplot as plt # plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Basic Data Structures in Pandas.\n",
    "\n",
    "Pandas provides two primary data structures for working with data: [__pandas.Series__](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) and [__pandas.DataFrame__](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "These structures are highly efficient for manipulating and analyzing large datasets, allowing for operations like filtering, grouping, and merging with minimal code.\n",
    "\n",
    "#### 2.2.1 Series\n",
    "\n",
    "A __pandas.Series__ is a one-dimensional labeled array that can hold any data type (e.g., integers, floats, strings, or even Python objects). It is similar to a list or array, but with an associated index, which makes it easier to access specific elements based on a label. Think of a Series as a single column of data.\n",
    "\n",
    "Here is how you can create a simple pandas.Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data\n",
    "data = [10, 20, 30, 40, 50]\n",
    "\n",
    "# pass the data into a pandas.Series object\n",
    "s = pd.Series(data)\n",
    "\n",
    "# Displaying the Series\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the numbers __0 to 4 represent the index of each element__, and the values are the data stored in the Series.\n",
    "\n",
    "##### 1.3.2.2 DataFrame\n",
    "\n",
    "A __pandas.DataFrame__ a two-dimensional, tabular data structure that contains rows and columns, similar to a spreadsheet or SQL table. Each column in a DataFrame is a Series, and the rows can be indexed by labels. DataFrames are flexible and can store data of different types (integers, floats, strings) across various columns. A DataFrame can be created from dictionaries, lists, or by reading in a file (e.g., a CSV file).\n",
    "\n",
    "Here’s how you can create a pandas.DataFrame with three columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data as dictionary where each key-value pair represents a column\n",
    "# the key of the dictionary will be used as the column name.\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 40],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n",
    "}\n",
    "\n",
    "# pas dictionary to DataFrame object\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# a Jupyter Notebook has the convenience that it prints out a the last variable. \n",
    "# Thus we can display the DataFrame in a nice format by just calling the variable\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the numbers __0 to 3 represent the index of each row__, and the values are the data stored in the DataFrame.\n",
    "\n",
    "### 2.2.3 Accessing Data in a DataFrame\n",
    "Once you have created a DataFrame, you can access the data using column labels or row indices. Here are a few examples:\n",
    "\n",
    "__1. Accessing a single column__:\n",
    "\n",
    "_Note_: When accessing a column of a DataFrame a pandas.Series is returned. The index of this series is the index of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access a column of a DataFrame you use the same notation as a for a dictionary. The column name functions as the key. Be careful with typos!\n",
    "# a column is a pandas.Series\n",
    "names_series = df['Name']\n",
    "\n",
    "# print the column\n",
    "print(\"The names_series contains: \\n{}\".format(names_series))\n",
    "\n",
    "# print the type\n",
    "print(\"\\nThe type of name_series is: {}\".format(type(names_series)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Accessing multiple columns__:\n",
    "\n",
    "_Note_: When accessing a multiple columns of a DataFrame a pandas.DataFrame is returned. The index of the DataFrame remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the 'Name' and 'City' columns\n",
    "# This will return a DataFrame object\n",
    "sub_df = df[['Name', 'City']]\n",
    "\n",
    "# printing the type of sub_df\n",
    "print(\"The type of the sub_df is: {}\".format(type(sub_df)))\n",
    "\n",
    "# printing the sub_df using jupyter notebook automatic print\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Accessing a row__:\n",
    "\n",
    "_Note_: When accessing a row of a DataFrame a pandas.Series is returned. __The index of this series are the column names__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the first row\n",
    "# This will return a pandas.Series\n",
    "first_row = df.iloc[0]\n",
    "\n",
    "print(\"The first row of the DataFrame is: \\n{}\".format(first_row))\n",
    "\n",
    "# print the type\n",
    "print(\"\\nThe type of first_row is: {}\".format(type(first_row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading, Visualizing and Grouping\n",
    "\n",
    "With these basics covered, you’re ready to start using pandas for more advanced data preparation tasks.\n",
    "\n",
    "Before we start looking into Biomedical datasets we will first use a more simpler dataset to get a bit more familiarized with pandas. We will have a look at the __Titanic dataset__. \n",
    "\n",
    "The dataset contains the following data:\n",
    "\n",
    "1. __PassengerId__: A unique identifier for each passenger.\n",
    "2. __Survived__: Indicates whether the passenger survived (1) or not (0).\n",
    "3. __Pclass__: The passenger's ticket class (1st, 2nd, or 3rd class), which is a proxy for socio-economic status.\n",
    "4. __Name__: The full name of the passenger.\n",
    "5. __Sex__: The gender of the passenger (male or female).\n",
    "6. __Age__: The age of the passenger. Some values are missing in this column.\n",
    "7. __SibSp__: The number of siblings or spouses the passenger had aboard the Titanic.\n",
    "8. __Parch__: The number of parents or children the passenger had aboard the Titanic.\n",
    "9. __Ticket__: The ticket number of the passenger.\n",
    "10. __Fare__: The fare paid by the passenger for the journey.\n",
    "11. __Cabin__: The cabin number where the passenger stayed. This column has many missing values.\n",
    "12. __Embarked__: The port where the passenger boarded the ship. It can take three values: C (Cherbourg), Q (Queenstown), or S (Southampton).\n",
    "\n",
    "The Titanic dataset is relatively simple and helps beginners understand key data manipulation techniques without being too overwhelming. It includes:\n",
    "\n",
    "* __Categorical data__: e.g., Sex, Embarked\n",
    "* __Numerical data__: e.g., Age, Fare\n",
    "* __Missing values__:  which are common in real-world datasets\n",
    "* __Label data__ : (Survived), useful for machine learning tasks like classification\n",
    "\n",
    "By exploring this dataset, you will learn how to load data into pandas, inspect it, and clean it, which are crucial first steps before applying machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loading Data \n",
    "\n",
    "Pandas provides multiple ways of loading data files, specialized in different formats.\n",
    "* [read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\n",
    "* [read_excel()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html)\n",
    "* [read_pickle()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_pickle.html)\n",
    "* [read_json()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html)\n",
    "* [read_html()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html)\n",
    "\n",
    "Depending on the structure of your data, pandas will load your data into a pandas.Series (if the data is one dimensional) or a pandas.DataFrame (if the data is multidimensional). In the case of the titanic dataset it will be loaded into pandas.DataFrame. \n",
    "\n",
    "The dataset is stored in the __\"Data\" folder__ of your project and is stored in __.csv__ format. Thus, we will use the __read_csv()__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file into a pandas.DataFrame\n",
    "df_titanic = pd.read_csv(\"../Data/Lab2_titanic.csv\")\n",
    "\n",
    "# printing the type of the entire DataFrame\n",
    "print(\"The type of a DataFrame is: {}\".format(type(df_titanic)))\n",
    "\n",
    "# show the first 5 rows of the DataFrame\n",
    "df_titanic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Getting to know the Data\n",
    "\n",
    "Once the data is loaded, it's important to explore it to understand the structure and contents. You can use a few pandas functions to get a quick overview of the dataset. The full pandas function library is quite extensive. You can check the [documentation](https://pandas.pydata.org/docs/) if you want.\n",
    "\n",
    "Pandas DataFrame and Series objects include a set of useful __attributes__ and __functions__, which we can use to explore the data.\n",
    "\n",
    "#### 3.2.1 Some Attributes\n",
    "\n",
    "__1. Getting the columns of the DataFrame__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns of the DataFrame\n",
    "df_columns = df_titanic.columns\n",
    "\n",
    "# print the columns\n",
    "print(\"The column names of the DataFrame are: \\n{}\".format(df_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Getting the types of each column in the dataset__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data types of each column\n",
    "df_titanic.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Getting the shape of the DataFrame__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of a DataFrame\n",
    "df_shape = df_titanic.shape\n",
    "\n",
    "# print the shape\n",
    "print('DataFrame shape: {}| number of rows: {} | number of columns: {}'.format(df_shape, df_shape[0], df_shape[1]))\n",
    "\n",
    "# alternatively you can get the number of rows by applying python's len() function to the DataFrame\n",
    "print('DataFrame number of rows: {} '.format(len(df_titanic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Some Functions\n",
    "\n",
    "__1. Displaying the first N rows of a DataFrame using the function:__\n",
    "* [head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 rows of a DataFrame\n",
    "df_titanic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Setting the index of the DataFrame with a column of the DataFrame using the function:__\n",
    "* [set_index()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a column as the index of the DataFrame (you just need to pass the name of the column. Be careful with typos!)\n",
    "df_titanic = df_titanic.set_index('PassengerId')\n",
    "\n",
    "# print the first 5 rows of a DataFrame\n",
    "df_titanic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Get the number of unique values and the corresponding unique values of a column using the functions:__\n",
    "* [nunique()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nunique.html)\n",
    "* [unique()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.unique.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of unique values of column 'Sex'\n",
    "print('Number of unique values of \\'Sex\\' column: {}'.format(df_titanic['Sex'].nunique()))\n",
    "\n",
    "# get corresponding unique values\n",
    "print(\"Corresponding unique values of \\'Sex\\' column: {}\".format(df_titanic['Sex'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Get an overview of the dataset, including the data types and any missing values using the function:__\n",
    "* [info()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html)\n",
    "\n",
    "_Note_: you can see that there are missing values for the columns 'Age', 'Cabin', and 'Embarked' as they do not reach the full number of rows (891)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the informaton of the DataFrame, in a neat print format.\n",
    "df_titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Get the overall statistics (e.g., mean, min, max, etc.) using the function:__\n",
    "* [describe()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)\n",
    "\n",
    "_Note_: using decribe() you can easily get an overall understanding of your data. For example you can see that the youngest passenger was less than 1 year old, while the oldest was 80 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the general statitstics of the DataFrame (for each column)\n",
    "df_titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. Getting the number of counts for each unique value inside a column using the function:__\n",
    "* [value_counts()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html)\n",
    "\n",
    "_Note_: In the example below we use value_counts() on the 'Sex' column. As the column has two unique values (male and female) the value_counts() function will count the number of male and female passengers in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of counts for each unique element inside a column\n",
    "# value_counts() sums up the number of values for each distinct entry of a column\n",
    "# the function retruns as pandas.Series object\n",
    "df_titanic['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Visualization\n",
    "\n",
    "Data visualization is an essential part of data analysis, as it allows you to understand your dataset more intuitively. With pandas, you can quickly create visualizations to get a deeper sense of your data before further processing or modeling. For this puropose the Pandas library includes some visualization tools. All pandas.Series and pandas.DataFrame objects have a [plot() function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html) that you can use to visualize the data.\n",
    "\n",
    "In this section, we will cover how to:\n",
    "\n",
    "* Visualize distributions and relationships in the dataset\n",
    "* Plot basic charts such as histograms, bar plots, and scatter plots\n",
    "\n",
    "There is plenty of other plots that you can explore that are not dicussed in this notebook. You can explore the entire plot library by checking out the documentation.\n",
    "\n",
    "_Note_: For plotting you need to import _matplotlib.pyplot_, which we did above in the import section\n",
    "\n",
    "#### 3.3.1 Visualizing Distributions\n",
    "\n",
    "Visualizing the distribution of numerical data is often the first step in understanding the spread and central tendency of a variable. A common way to visualize distributions is with a histogram. A histogram shows the distribution of a numerical variable by dividing it into \"bins\" and counting how many observations fall into each bin. You can plot a histogram by using:\n",
    "* [plot(kind='hist',...)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a histogram of the 'Age' column\n",
    "df_titanic['Age'].plot(kind='hist', bins=20, title='Age Distribution')\n",
    "\n",
    "# adding labels\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Bar Plots for Categorical Data\n",
    "For categorical data, such as gender or passenger class, a bar plot is often more appropriate. Bar plots visualize the count of occurrences for each category. You can generate bar plots by using:\n",
    "\n",
    "* [plot(kind='bar',...)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the number of counts within the Pclass column\n",
    "\n",
    "# get the value counts for the three classes\n",
    "passenger_class_counts = df_titanic['Pclass'].value_counts()\n",
    "\n",
    "# plot the data \n",
    "passenger_class_counts.plot(kind='bar', title='Class Count')\n",
    "\n",
    "\n",
    "# add title labels\n",
    "plt.xlabel('Pclass')\n",
    "plt.ylabel('No. Passangers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: The code above sorts the plots the according to their occurnce (from highest to lowest). In case you want to have the plot in the correct order you need to sort the indexes of the series we obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you want to have the plot in the correct order you need to sort the indexes of the series we obtained above\n",
    "p_class_counts_sorted = passenger_class_counts.sort_index()\n",
    "\n",
    "p_class_counts_sorted.sort_index().plot(kind='bar', title='Class Count')\n",
    "plt.xlabel('Pclass')\n",
    "plt.ylabel('No. Passangers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Grouping Variables\n",
    "\n",
    "The Pandas [groupby()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) function is a powerful tool for grouping data by a specific column and performing aggregate calculations, such as computing the mean, sum, or count of a column for each group. This is particularly useful when you want to break down the data into subsets to observe patterns or trends within categories.\n",
    "\n",
    "#### 3.4.1 Grouping Data by a Single Column\n",
    "\n",
    "The most common use of  [groupby()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) is to group data by a single column and then perform aggregation. For example, you might want to know the average fare paid by passengers in each class. You can do this by applying the following function to the _groupby object_:\n",
    "\n",
    "* [mean()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Pclass' and calculating the average fare\n",
    "avg_fare_by_class = df_titanic.groupby('Pclass')['Fare'].mean()\n",
    "\n",
    "# print the result\n",
    "print(avg_fare_by_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Grouping by Multiple Columns\n",
    "\n",
    "You can also group data by more than one column. For example, you might want to know the survival rate for different classes of 'PClass' and the 'Survived' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the sub DataFrame by the passenger ticket class and calculate the mean\n",
    "# as the surviver column is binary encoded, calculating the mean over it gives you the survivor percentage (survival rate)\n",
    "survival_rate_by_pclass = df_titanic[['Pclass', 'Survived']].groupby('Pclass').mean()\n",
    "\n",
    "# printing the result\n",
    "print(survival_rate_by_pclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot these results if you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "survival_rate_by_pclass.plot(kind='bar', title='Survival rate by Passenger Ticket Class')\n",
    "\n",
    "# add labels\n",
    "plt.xlabel(\"Passenger Ticekt Class\")\n",
    "plt.ylabel(\"Survival Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.3 Applying Multiple Aggregate Functions\n",
    "Sometimes, you might want to apply multiple aggregate functions to a grouped dataset. For instance, you could calculate both the mean and the standard deviation of passenger fares for each class. This returns both the average fare and the standard deviation of fares for each class.\n",
    "\n",
    "You can use this by applying the following function to the _groupby object_:\n",
    "* [agg()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying multiple aggregate functions\n",
    "fare_stats = df_titanic.groupby('Pclass')['Fare'].agg(['mean', 'std'])\n",
    "print(fare_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Logical Operations on DataFrames\n",
    "\n",
    "Logical operations in pandas allow you to filter your DataFrame based on conditions. This is a powerful tool for data analysis, as it enables you to focus on specific rows of data that meet certain criteria. You can think of it as asking questions about your data, like \"Which passengers paid more than $100 for their fare?\" or \"Which patients have an abnormal heart rate?\"\n",
    "\n",
    "### 4.1 Filtering Data Using a Single Condition\n",
    "You can filter rows of a DataFrame by specifying a condition. __Logical operators__ include:\n",
    "\n",
    "* <b>></b>: greater than\n",
    "* <b><</b>: less than\n",
    "* <b>>=</b>: greater than or equal to\n",
    "* <b><=</b>: less than or equal to\n",
    "* <b>==</b>: equal to\n",
    "* <b>!=</b>: not equal to\n",
    "\n",
    "You can apply these operators to any column to filter data.\n",
    "\n",
    "For example, to find all passengers who are older than 50 years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows where 'Age' is greater than 50. This will return a DataFrame \n",
    "df_older_passengers = df_titanic[df_titanic['Age'] > 50]\n",
    "\n",
    "# printing the first 5 rows of the DataFrame (You can see that the age column only contains values above 50)\n",
    "df_older_passengers.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Filter Data Using Multiple Conditions\n",
    "For combining multiple conditions you can use the following logical operators:\n",
    "\n",
    "* __AND (&)__: Both conditions must be true.\n",
    "* __OR (|)__: At least one condition must be true.\n",
    "* __NOT (~)__: Negates the condition.\n",
    "\n",
    "For example, to find passengers who were male __and__ paid more than $100 for their ticket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows where 'Fare' is greater than 100 or 'Pclass' is 1\n",
    "df_male_or_high_fare = df_titanic[(df_titanic['Sex'] == 'male') & (df_titanic['Fare'] > 100)]\n",
    "\n",
    "# print the number of passengers that were male and paid more than $100 for their ticket\n",
    "print(\"Nr. of passengers that were male and paid more than $100: {}\".format(df_male_or_high_fare.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Using isin() for Filtering with Lists of Values\n",
    "The [isin()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html) function allows you to filter rows based on whether a value in a column belongs to a list of values. For example, you might want to find all passengers who embarked from either \"C\" or \"Q\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows where 'Embarked' is either 'C' or 'Q'\n",
    "df_embarked_c_or_q = df_titanic[df_titanic['Embarked'].isin(['C', 'Q'])]\n",
    "\n",
    "# print the result\n",
    "print(\"Nr. of people that embarked in Cherbourg or Queenstown: {}\".format(df_embarked_c_or_q.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling Missing Data\n",
    "\n",
    "One of the most common challenges in data preparation is dealing with missing values. __Missing data can occur for many reasons such as data entry errors, lost records, or simply because some information was unavailable__. Properly handling missing data is crucial because many machine learning algorithms cannot handle them directly.\n",
    "\n",
    "### 5.1 Identifying Missing Data\n",
    "To begin, it’s important to know where the missing values are in the dataset. pandas provides several functions to help with this.\n",
    "\n",
    "__1. Checking for missing values using the function:__\n",
    "* [isnull()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isnull.html)\n",
    "\n",
    "In the example below we can see that the first, third, and fourth values of the 'Cabin' column are missing as there value returned value is _True_.\n",
    "\n",
    "_Note_: The term used for missing (or non-defined) data in programming is __null__. The is_null() function returns the booleans _True_ for missing values and _False_ for non-missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "df_titanic.isnull().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Summing missing values using the function:__\n",
    "* [sum()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sum.html)\n",
    "\n",
    "From the output below you can see again, that the columns 'Age', 'Cabin', and 'Embarked' have missing values.\n",
    "\n",
    "_Note_: Using sum on booleans returns the number of _True_ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting missing values in each column\n",
    "# this will return a pandas.Series where the index are the column names of the DataFrame\n",
    "df_titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 What to do with Missing Data\n",
    "\n",
    "Once we’ve identified missing values, we need to decide how to handle them. The two main approaches are:\n",
    "\n",
    "1. __Dropping missing data__: Removing rows or columns with missing values\n",
    "2. __Imputing missing data__: Filling in missing values\n",
    "\n",
    "#### 5.2.1 Dropping Missing Data\n",
    "\n",
    "If a column or row has too many missing values, or the missing values are not essential, you might want to drop them from the dataset entirely. You can do this using the dropna() function.\n",
    "\n",
    "__1. Dropping rows with missing values, where at least one column in the row contains a missing value using:__\n",
    "* [dropna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)\n",
    "\n",
    "_Note_: In cases where a certain column contains a lot of missing values using dropna() along the rows will result in a highly reduced dataset. As you can see below before dropping we had 891 rows of data and after dropping we only have 183. In these cases it might make more sense to remove the column with the missing data instead of dropping the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the number of rows before dropping rows with missing values\n",
    "print(\"Number of rows before dropping rows containing missing values: {}\".format(df_titanic.shape[0]))\n",
    "\n",
    "# Dropping rows that have any missing values\n",
    "df_titanic_dropped_rows = df_titanic.dropna()\n",
    "\n",
    "# printing the number of rows after dropping rows with missing values\n",
    "print(\"Number of rows after dropping rows containing missing values: {}\".format(df_titanic_dropped_rows.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Dropping columns with missing values:__\n",
    "\n",
    "* [dropna(axis=1)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)\n",
    "\n",
    "_Note_: to drop columns you need to pass 1 to the _axis_ parameter of the dropna() function. The 1 corresponds to the dimension of the DataFrame. In line with programming we usually start to count at zero meaning that the __rows are dimnension 0 while the columns are dimension 1__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the number of columns before dropping containing missing values\n",
    "print(\"Number of columns before dropping columns containing missing values: {}\".format(df_titanic.shape[1]))\n",
    "\n",
    "# Dropping columns that have missing values\n",
    "titanic_cleaned_cols = df_titanic.dropna(axis=1)\n",
    "\n",
    "# printing the number of after before dropping columns containing missing values\n",
    "print(\"Number of columns before dropping columns containing missing values: {}\".format(titanic_cleaned_cols.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Imputing Missing Data\n",
    "Instead of dropping rows or columns, a better strategy is often to fill in missing values with a placeholder or a calculated value. This is called imputation. \n",
    "\n",
    "> However, __imputation should always be done with great care and only for variables where it actually makes sense as imputation can introduce bias into your dataset if not done carefully, especially if a large proportion of values are missing__.\n",
    "\n",
    "> Furthermore, __to ensure that no data leakage can occur, data imputation should only be perfromed AFTER the data has been split into training and test set__. See sections 10-11 for more information.\n",
    "\n",
    "__1. Filling missing values with a constant: You can replace missing values with a specific constant (e.g., 0, or \"Unknown\" for categorical variables) using:__\n",
    "\n",
    "* [fillna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)\n",
    "\n",
    "_Note_: In the case of the titanic dataset it is not really useful to replace the age with 0 as done below. The code sample below just presents how to perform this type of imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing 'Age' values with 0\n",
    "df_titanic_filled_zero = df_titanic.fillna({'Age': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Filling missing values with the mean/median/mode: For numerical columns, in specific cases it might be useful to replace missing values with the mean or median of the column using:__\n",
    "* [fillna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)\n",
    "\n",
    "_Note_: In the case of the titanic dataset it is not really useful to replace the age with median as done below. The code sample below just presents how to perform this type of imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing 'Age' values with the median\n",
    "median_age = df_titanic['Age'].median()\n",
    "df_titanic_filled_median = df_titanic['Age'].fillna(median_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Data Cleaning\n",
    "Once you've dealt with missing data, the next step in preparing your data for analysis or machine learning is __data cleaning__. Data cleaning involves correcting or removing data that is incorrect, out-of-date, redundant, or formatted inconsistently. Clean data ensures that models can learn effectively from the data without being misled by errors or noise.\n",
    "\n",
    "#### 6.1 Removing Duplicates\n",
    "\n",
    "Duplicate rows can distort your analysis and cause bias in machine learning models. Duplicates can occur due to errors in data entry, repeated records, or other issues. pandas provides an easy way to identify and remove duplicates.\n",
    "\n",
    "_Note_: The titanic dataset does not have any duplicated rows. The code below just shows how to search for and delete duplicated rows.\n",
    "\n",
    "__1. Finding duplicate rows by using the function:__\n",
    "* [duplicated()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate rows\n",
    "duplicates = df_titanic.duplicated()\n",
    "\n",
    "# print the sum of duplicated rows. This equals to the number of duplicated rows\n",
    "print(duplicates.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Removing duplicate rows by using the function:__\n",
    "* [drop_duplicates()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicate rows\n",
    "titanic_no_duplicates = df_titanic.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Fixing Inconsistent Data Formats\n",
    "Data can be entered in various formats, especially when working with categorical or textual data. For instance, one column might contain values like \"Male\" and \"male,\" which pandas would treat as different categories. It's essential to standardize these formats to avoid errors in analysis.\n",
    "\n",
    "__1. Standardizing text data: Convert text data to a consistent format (e.g., lowercase) to avoid inconsistencies by using the function:__\n",
    "* [str()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.html)\n",
    "\n",
    "_Note_: The str() function can call all functions that strings can use (e.g., lower(), split(), replace(), etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'Sex' column to lowercase\n",
    "df_titanic['Sex'] = df_titanic['Sex'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Fixing categorical data inconsistencies: If you notice inconsistencies in categorical variables (e.g., both \"M\" and \"male\" are used to represent the same category), you can use the following function for standardization__:\n",
    "* [replace()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing 'M' with 'male' and 'F' with 'female'\n",
    "df_titanic['Sex'] = df_titanic['Sex'].replace({'M': 'male'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "Feature Engineering is the process of transforming raw data into features that better represent the underlying patterns and relationships for machine learning models. The quality and quantity of the features you provide to a model can significantly affect its performance.\n",
    "\n",
    "Models often benefit from new infered features, created from the original data. Pandas allows to apply mathematical operation on its objects (pandas.Series and pandas.DataFrame).\n",
    "\n",
    "In this section, we will cover:\n",
    "\n",
    "* Why feature engineering is important\n",
    "* Creating new features\n",
    "* Transforming existing features\n",
    "\n",
    "### 7.1 Why Feature Engineering is Important\n",
    "__Raw data often contains noise, irrelevant information, or incomplete variables that don't fully capture the relationships needed for machine learning__. Feature engineering helps create more useful features that can make patterns more visible and machine learning models more effective.\n",
    "\n",
    "In the context of biomedical data, well-engineered features can represent important medical concepts like patient risk scores, treatment histories, or vitals trends, which can directly improve prediction accuracy.\n",
    "\n",
    "### 7.2 Creating New Features\n",
    "Creating new features from existing data can reveal important relationships that weren’t obvious in the raw data.\n",
    "\n",
    "For example, lets get the number of family members each passenger had by adding the following two columns:\n",
    "* __SibSp__: The number of siblings or spouses the passenger had aboard the Titanic.\n",
    "* __Parch__: The number of parents or children the passenger had aboard the Titanic.\n",
    "\n",
    "As the variable __SibSp__ encodes the number of siblings and spouses, and the variable __Parch__ encodes the number of parents and children, we can infer the size of the family.\n",
    "\n",
    "We can add a new column to the DataFrame by just defining a new column name and assigning it a value. Be aware that new assigned values have to have the same number of row as the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a new column to the DataFrame by mathematically adding two columns\n",
    "df_titanic['FamilySize'] = df_titanic[\"SibSp\"] + df_titanic[\"Parch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Transforming Existing Features\n",
    "Feature transformations can often improve the predictive power of a dataset by converting non-linear relationships into linear ones or scaling features to a common range.\n",
    "\n",
    "__1. Log transformations: Taking the logarithm of a feature can help normalize data and reduce the influence of outliers__.\n",
    "\n",
    "For example, we can log-transform the \"Fare\" column to deal with large outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # log1p is used to avoid log(0)\n",
    "df_titanic['LogFare'] = np.log1p(df_titanic['Fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Binning numerical variables: Sometimes, converting continuous variables into categories or bins can make relationships clearer. For example, we could group passengers into age ranges. You can do this by using the function__:\n",
    "\n",
    "* [cut()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html)\n",
    "\n",
    "_Note_: The example below categorizes the data into right-inclusive bins. The bins and the corresponding categories are the following:\n",
    "* (0, 12]: Values greater than 0 and up to and including 12 are categorized as 'Child'.\n",
    "* (12, 18]: Values greater than 12 and up to and including 18 are categorized as 'Teen'.\n",
    "* (18, 40]: Values greater than 18 and up to and including 40 are categorized as 'Adult'.\n",
    "* (40, 60]: Values greater than 40 and up to and including 60 are categorized as 'Middle-Aged'.\n",
    "* (60, 100]: Values greater than 60 and up to and including 100 are categorized as 'Senior'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning 'Age' into categories\n",
    "df_titanic['AgeGroup'] = pd.cut(df_titanic['Age'], bins=[0, 12, 18, 40, 60, 100], labels=['Child', 'Teen', 'Adult', 'Middle-Aged', 'Senior'])\n",
    "\n",
    "# printing the first 10 rows of the DataFrame. Take a look at the new columns 'FamilySize','LogFare', and 'AgeGroup'\n",
    "df_titanic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pandas Profiling\n",
    "\n",
    "[Pandas Profiling](https://pandas-profiling.ydata.ai/docs/master/index.html) is a python library which generates insightful reports on Pandas datasets.\n",
    "\n",
    "Note: You (may) need to install it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this line to install the package through pip\n",
    "# !pip install pandas-profiling \n",
    "\n",
    "# alternatively you can also use the conda prompt as described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile = ProfileReport(df_titanic, title=\"Titanic Profiling\", vars={\"num\": {\"low_categorical_threshold\": 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile.to_file(\"Data/titanic_dataset.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Exercises on Pandas Functionalities\n",
    "\n",
    "In the following exercises we will explore a dataset commonly used for binary classification tasks in materials science. The dataset contains measurements collected from a set of 699 material samples, each representing a different specimen analyzed in a laboratory environment. Every sample is described by 10 quantitative attributes, each representing a physical property obtained through standard characterization techniques such as microscopy, spectroscopy, or optical measurements.\n",
    "\n",
    "These attributes include parameters such as density, grain size uniformity, crystal symmetry, impurity count, optical scattering, and defect density. All features are encoded as integer values ranging from 1 to 10, representing normalized or discretized measurements.\n",
    "\n",
    "The goal of the classification task is to determine the material class of each sample. The dataset includes two categories, representing two distinct types of materials with different structural or optical characteristics\n",
    "\n",
    "\n",
    "The datset has the following columns:\n",
    "\n",
    "1. __Sample_ID__: A unique ID for each instance.\n",
    "2. __Density__: mass/volume.\n",
    "3. __Grain Size Uniformity__: Measured with SEM (Scanning Electron Microscopy).\n",
    "4. __Crystal Symmetry Index__: Measured with X-Ray difraction.\n",
    "5. __Surface Adhesion Coefficient__: Measured with contact angle measurements.\n",
    "6. __Average Grain Size__: Measured with SEM.\n",
    "7. __Impurity Count__: Mass spectrometry (this feature may have missing values).\n",
    "8. __Optical Scattering Index__: Measured with laser scattering experiments using a integrating sphere.\n",
    "9. __Reflectivity__: Measured with a Spectrophotometer .\n",
    "10. __Defect Count__: Measured with SEM.\n",
    "11. __Material Class__: (2) low quality material; (4) high quality material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1__:\n",
    "* Load the Lab2_Material_Dataset.csv from UCI Repository\n",
    "* Display the first 10 rows of the Dataset. \n",
    "\n",
    "__Hints__:\n",
    "* Hint: use pandas [pd.read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2__: As you can see, the dataset comes without any column names. Therefore you need to set the columns names with the names defined above.\n",
    "\n",
    "* Store the columns defined above into a list of strings.\n",
    "* Override the columns of the DataFrame using the list.\n",
    "* Display the first 5 columns of the DataFrame to ensure that the column names were actually updated.\n",
    "\n",
    "__Hints__:\n",
    "* Hint: The __.columns__ attribute can also be used to set new column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3__: There are some rows that are duplicated in the DataFrame, meaning that the entire row of these instances is exactly the same. Having multiple instances of the same data may introduce bias when training a model. Therefore, these need to be removed. But before:\n",
    "\n",
    "* Find how many duplicated rows exist in the DataFrame.\n",
    "\n",
    "__Hints__:\n",
    "* Hint 1: pandas.Series and pandas.DataFrame objects have a [sum()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sum.html) function that you can use to get the number of duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4__: As you now know that there are duplicate rows in the DataFrame, you need to remove these lines.\n",
    "\n",
    "* Print the number of rows that the DataFrame has at the moment.\n",
    "* Drop duplicate lines in the DataFrame.\n",
    "* Print the number of lines that the DataFrame has after dropping the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 5__: Even though we removed duplicate lines there are still some instances where the __Sample ID__ column has the same number, but the rest of the data (the other columns) is different. These instances of data can be considered __noisy data__, because they point to the same sample but have different values for this sample. It is always necessary to remove __noisy data__, as this will lead to sub-optimal training of a model. \n",
    "\n",
    "* Check if Sample ID is actually unique\n",
    "\n",
    "__Hints__: \n",
    "* Hint 1: You can do that by looking at the __number of unique__ values inside the 'Sample ID' column and compare that to the total number of samples in the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 6__: Now that you know that there a these instances where the Sample_ID is duplicated, you need to find out which of the rows have the same Sample_ID. For that you need to get the index of these rows. \n",
    "\n",
    "* Get the indices of the rows where 'Sample ID' column has duplicated values\n",
    "\n",
    "__Hints__:\n",
    "* check the [pandas.duplicated()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html) function. It can be used with a certain parameter so that it only looks for duplicates in a certain column.\n",
    "* The returned boolean pandas.Series can be used to retrieve only the rows containing duplicates from the DataFrame. \n",
    "* In the introduction section there is an example on how to get the __index__ of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 7__: Now you need to remove those rows containing the __noisy data__\n",
    "\n",
    "* Print the number of rows that the DataFrame has at the moment.\n",
    "* Drop rows containing the duplicated data.\n",
    "* Print the number of rows after dropping the rows to ensure you actually dropped the rows.\n",
    "\n",
    "__Hints__:\n",
    "* Hint 1: You can either use the indices you obtained by passing them to the [drop()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) function\n",
    "* Hint 2: Or you can use [drop_duplicates()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html) and use a particular parameter of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 8__: Set Sample ID as dataframe index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 9__: For some reason the __Impurity Count__ column is read as an _object_ event though it should contain only integer values. There is a particular reason for this as the researcher compiling the dataset set a specific value if the data was missing.\n",
    "\n",
    "* Check column types using `df.info()` and confirm that the Type for __Impurity Count__ is acutally _object_.\n",
    "* Find out which is the value that the research set for missing data.\n",
    "\n",
    "__Hints__: \n",
    "* There is a code example provided above that might help you finding this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 10__: Now that you know which is the value the researcher used for missing values you can clean your data by removing rows that contain the missing value in the __Impurity Count__ column. \n",
    "* Remove Sample IDs with missing values\n",
    "* Convert Impurity Count to Type int\n",
    "* Get the info() of the DataFrame after converting the column to int and check if all types are now _int64_ or _int32_.\n",
    "\n",
    "__Hints__:\n",
    "* Hint 1: This can be solved using a logical operation\n",
    "* Hint 2: Check the function [astype()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) for converting the column to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Features, Target Class and Splitting data\n",
    "\n",
    "Before building a machine learning model, it’s important to split your dataset into separate training and testing sets. This allows you to evaluate how well your model performs on new, unseen data. But before we split our data we need to define the following:\n",
    "\n",
    "\n",
    "1. The features/variables we are going to pass to the model as input\n",
    "2. The target class we want to predict using our model (i.e., the output of the model)\n",
    "\n",
    "For this section we will use the [scikit-learn library](https://scikit-learn.org/stable/index.html). It provides functions for all the steps we need to perform.\n",
    "\n",
    "_Note_: If you haven't installed scikit-learn yet, you can install it using pip or conda as described at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necesary function from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Understanding Features and the Target Class\n",
    "In a machine learning model, we work with two key components: __features__ (also called predictors or independent variables) and the __target class__ (also called the dependent variable or label). Understanding the distinction between these is crucial for framing and solving a predictive task.\n",
    "\n",
    "#### 10.1.1 What are Features?\n",
    "Features are the input variables or attributes that describe the data. These can be numerical values (e.g., age, salary) or categorical values (e.g., gender, city). In the context of machine learning, the model uses these features to learn patterns and make predictions.\n",
    "\n",
    "For example, in the Titanic dataset, some common features we might use to predict whether a passenger survived or not include:\n",
    "\n",
    "* __Pclass__: The class of the ticket (1st, 2nd, or 3rd class)\n",
    "* __Age__: The age of the passenger\n",
    "* __Fare__: The amount paid for the ticket\n",
    "* __Sex__: Gender of the passenger (which we'll encode numerically)\n",
    "* __Embarked__: The port from which the passenger boarded the Titanic (which we'll also encode numerically)\n",
    "\n",
    "\n",
    "#### 10.1.2 What is the Target Class?\n",
    "The target class (or simply target) is the variable we aim to predict based on the features. In classification tasks, the target is often a categorical value representing different classes. In regression tasks, the target is typically a continuous numerical value.\n",
    "\n",
    "For the Titanic dataset, the target class is:\n",
    "\n",
    "* __Survived__: A binary variable indicating whether a passenger survived (1) or not (0).\n",
    "\n",
    "\n",
    "#### 10.2.3 Defining Features and the Target in pandas\n",
    "When preparing data for a machine learning model, we separate the features and the target. Using pandas, this is done by creating two variables:\n",
    "\n",
    "* __X__: Contains the features (predictor variables).\n",
    "* __y__: Contains the target class (the value we want to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows where column 'Age' is NaN\n",
    "df_titanic = df_titanic.dropna(subset=['Age'])\n",
    "\n",
    "# dropping rows where 'Embarked' is NaN\n",
    "df_titanic = df_titanic.dropna(subset=['Embarked'])\n",
    "\n",
    "# Define the features (X) and the target (y)\n",
    "X_titanic = df_titanic[['Pclass', 'Age', 'Fare', 'Sex', 'Embarked']]  # Add other features as needed \n",
    "y_titanic = df_titanic['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Splitting the Data into Different Sets\n",
    "When building machine learning models, it's essential to split your dataset into separate parts to ensure you can evaluate your model's performance properly. The most common splits are:\n",
    "\n",
    "* __Training Set__: The portion of the data the model learns from.\n",
    "* __Validation Set__: A smaller part of the training data used for tuning model parameters and preventing overfitting.\n",
    "* __Testing Set__: The final portion of data used to evaluate the model's performance on unseen data.\n",
    "\n",
    "<div>\n",
    "<img src=\"..\\images\\train_test.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Depending on the use case and the characteristics of the dataset, the data scientist (aka you!) must decide which approach to use to ensure the most reliable results.\n",
    "\n",
    "#### 10.2.1 Train-Test Split\n",
    "\n",
    "In simpler workflows, data is often split into just training and testing sets. This ensures the model is evaluated on data it has not seen during training. Usual poprotions of Train and Test sets are:\n",
    "* __Training Set (70 - 90 %)__: Used to fit the model.\n",
    "* __Testing Set (10 - 30 %)__: Used to evaluate the final model performance.\n",
    "\n",
    "#### 10.2.2 Train-Validation-Test Split\n",
    "If it is also needed to fine-tune some hyperparameters you can also use the following split:\n",
    "\n",
    "* __Training Set (e.g., 60% of the data)__: Used to fit the model.\n",
    "* __Validation Set (e.g., 20% of the data)__: Used for tuning hyperparameters and selecting the best model. The model evaluates itself against this set during training to avoid overfitting.\n",
    "* __Testing Set (e.g., 20% of the data)__: Used to evaluate the final model performance after tuning is complete.\n",
    "\n",
    "This ensures that you have a fair estimate of how the model performs on new data without overfitting or bias from the validation step.\n",
    "\n",
    "### 10.3 A Common Pitfall: Data Leakage\n",
    "When splitting our data into their repsective sets we need to be careful not to cause __data leakage__. Data leakage occurs when information from outside the training dataset is used to train the model, leading to overly optimistic performance estimates. This can happen when the model is inadvertently trained on data or information that it should not have access to, thereby invalidating the model's ability to generalize to unseen data. Data leakage can occur in several ways:\n",
    "\n",
    "1. __Data splitting__: If the data is not split properly, for example in such a way that some test data leaks into the training set, it can result in a model that seems to perform well during validation but fails in practical applications.\n",
    "\n",
    "2. __Data scaling__: When scaling the data (see section 11) before splitting it into the respective sets, the calculated scaling parameters contain information from the test data, thus giving the model access to information it shouldn't have.\n",
    "\n",
    "3. __Data encoding__: When encoding data (e.g., categorical data to numerical data, see section 10.5), the encoding should be applied separately to the training and test set. While the potential of data leakage with respect to encoding is in most cases low, it can still occur when the encoding is more complex (e.g., encoding of variables in timeseries forecasting tasks).\n",
    "\n",
    "4. __Data imputation__: Data imputation is mostly done using information from the data points contained in the dataset (e.g., replacing missing data with the median, mean, etc.). In the same vein as scaling, imputation should be done after splitting the data.\n",
    "\n",
    ">__Generally speaking, it is always a good practice to view the training, validation, and testing datasets as separate datasets. All transformations should always be fit on the training set, and then the parameters of the fit should be used to transform the other sets (validation and test)__. \n",
    "\n",
    "### 10.4 How to Split Data in scikit-learn\n",
    "For splitting the data into Train and Test sets we can use scikit-learn. Luckily scikit-learn will perform the data splitting for us in such a way that data leakage realted to data splitting does not occur. For this the following function can be used:\n",
    "* [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "_Note_: In the example below we use a 80/20 split for train/test\n",
    "\n",
    "_Note_: The train_test_split() function returns 2 pairs of variables (4 in total), the X and y for training and the X and y for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets: 80% train_val, 20% test (it is only necessary to define the test size)\n",
    "X_train_titanic, X_test_titanic, y_train_titanic, y_test_titanic = train_test_split(X_titanic, y_titanic, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 Encoding of Categorical Variables\n",
    "Endcoding of categorical variables is a part of __Feature Engineering__. Machine learning models often require categorical variables to be represented numerically.\n",
    "\n",
    ">__To avoid data leakage during the encoding process we fit the encoder on the training set and then use the same encoder to transform the validation and test sest__.\n",
    "\n",
    "In scikit-learn all objects that apply some kind of transformation (e.g., encoding, scaling, imputation, etc.) to the data, have convenience functions that help us avoiding data leakage. These are:\n",
    "\n",
    "* __fit()__: for fittinf the object (e.g., encoder, scaler, imputer, etc.) to the data. __This function should only be used on the training set__.\n",
    "* __fit_transfrom()__: for fitting the object to the data and transforming data in a single function call. __This function should only be used on the training set__.\n",
    "* __transform()__: for transforming data using the parameters that were obtained during the fitting process. __This function should be used on the validation and test sets, or the training set if before only the fit() function was used__.\n",
    "* __inverse_transform__: for performing the inverse transformation, i.e., obtaining the original data. This function (if neeeded) can be used on all datasets (training, validation, and test),\n",
    "\n",
    "The scikit-learn package offers a variety of different encoder types. We are going to have a look a the two most commonly used.\n",
    "\n",
    "#### 10.5.1 Label Encoding\n",
    "This replaces each category with a unique number. For example, in the Titanic dataset, the column 'Sex' needs to be encoded as integers 'male': 0 and 'femal':1.\n",
    "\n",
    "<div>\n",
    "<img src=\"..\\images\\ordinal.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "To perform this encoding we will use scikit-learn's:\n",
    "* [LabelEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the LabelEncoder\n",
    "l_encoder = LabelEncoder()\n",
    "\n",
    "# get the \"Sex\" column before encoding \n",
    "# (this line is not needed it is just to show you the difference before and after encoding at the end of this code block)\n",
    "orig_sex_col = X_train_titanic['Sex']\n",
    "\n",
    "# fit the encoder using the training data and transform the 'Sex' column (here we are directly overriding the column with the encoded values)\n",
    "X_train_titanic['Sex'] = l_encoder.fit_transform(X_train_titanic['Sex'])\n",
    "\n",
    "# transform the test data using the label encoder that was fit on the training data\n",
    "X_test_titanic['Sex'] = l_encoder.transform(X_test_titanic['Sex'])\n",
    "\n",
    "# displaying difference before and after\n",
    "print(\"first 5 values of \\'Sex\\' column before encoding {}\".format(orig_sex_col.head(5)))\n",
    "print(\"first 5 values of \\'Sex\\' column after encoding {}\".format(X_train_titanic['Sex'].head(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.5.2 One-Hot Encoding\n",
    "One-hot encoding is a process used to convert categorical variables into a format that can be provided to machine learning algorithms. Instead of assigning a single number to each category, one-hot encoding creates new binary columns—one for each unique category. Each column contains a 1 if the category is present and a 0 otherwise. This avoids giving numerical meaning or rank to the categories, which could mislead machine learning models.\n",
    "\n",
    "<div>\n",
    "<img src=\"..\\images\\one_hot.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "For example, in the Titanic dataset, the column 'Embarked' needs to be one-hot encoded. This will result in three new columns 'Embarked C', 'Embarked_Q', 'Embarked_S'.\n",
    "\n",
    "To perform this encoding we will use scikit-learn's:\n",
    "* [OneHotEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "\n",
    "_Note_: When we use one-hot encoding to convert categorical variables into a numerical format, each unique category is represented by a separate binary column. For example, the 'Embarked' column in the Titanic dataset has three categories: 'C' (Cherbourg), 'Q' (Queenstown), and 'S' (Southampton).\n",
    "\n",
    "With one-hot encoding, we create new columns for each category, where each column contains a 1 if the category is present and a 0 if it is not. However, to avoid a problem called the __dummy variable trap__, we often choose to drop one of these columns. The dummy variable trap occurs when the model receives redundant information from all the encoded columns, which can lead to multicollinearity and inflated variance in our model’s estimates.\n",
    "\n",
    "For simplicity, __we do not consider the dummy variable trap in the code below__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OneHotEncoder without dropping the first category\n",
    "# encoder = OneHotEncoder(sparse_output=False)  # run this line of code if your scikit-learn version is > 1.2.0\n",
    "oh_encoder = OneHotEncoder(sparse_output=False)  # # run this line of code if your scikit-learn version is < 1.2.0\n",
    "\n",
    "# fit the encoder on the training data and tranform it (this will return a numpy.array)\n",
    "embarked_encoded_train = oh_encoder.fit_transform(X_train_titanic[['Embarked']])\n",
    "\n",
    "# transform the test data using the encoder that was fit on the training data\n",
    "embarked_encoded_test = oh_encoder.transform(X_train_titanic[['Embarked']])\n",
    "\n",
    "# convert the result into a DataFrame for easier handling\n",
    "df_encoded_train = pd.DataFrame(embarked_encoded_train, columns=oh_encoder.get_feature_names_out(['Embarked']))\n",
    "df_encoded_test = pd.DataFrame(embarked_encoded_test, columns=oh_encoder.get_feature_names_out(['Embarked']))\n",
    "\n",
    "# concatenate the new one-hot encoded columns to the c\n",
    "X_train_titanic = pd.concat([X_train_titanic.reset_index(drop=True), df_encoded_train], axis=1)\n",
    "X_test_titanic = pd.concat([X_test_titanic.reset_index(drop=True), df_encoded_test], axis=1)\n",
    "\n",
    "# display the first 5 rows the training data\n",
    "# (This line is not needed, it is just to show the difference before and after encoding)\n",
    "print(X_train_titanic[['Embarked', 'Embarked_C', 'Embarked_Q', 'Embarked_S']].head(5))\n",
    "\n",
    "# drop the original 'Embarked' column as it is not needed anymore\n",
    "X_train_titanic = X_train_titanic.drop(columns='Embarked')\n",
    "X_test_titanic = X_test_titanic.drop(columns='Embarked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the first 10 rows of the DataFrame containing the training data after finishing encoding\n",
    "X_train_titanic.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6. Exercises\n",
    "\n",
    "__Exercise 1__:  Select the X and y data for the Material Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2__: Divide the Material dataset into training and testing set using a 70/30 train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3__: Encode the target variable of the Material Dataset. \n",
    "\n",
    "As the Material Dataset uses the following values:\n",
    "* __low quality__: 2\n",
    "* __high quality__: 4\n",
    "\n",
    "A __label encoding__ of the data is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Scaling\n",
    "\n",
    "__Feature scaling__ is crucial in machine learning because it ensures that all features contribute equally to the model, preventing any single feature with a larger range of values from dominating the learning process. Many algorithms, such as K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Gradient Descent-based models (i.e., Deep Learning models), rely on the distance between data points. If features are on different scales, it can distort the distances and lead to poor performance. Scaling also speeds up the convergence of optimization algorithms, making training more efficient. Overall, feature scaling improves model accuracy and training stability.\n",
    "\n",
    "Scikit-learn also provides the functionality for scaling data. The two main scalsers are (there are other scalers that exist that are not discussed in this Notebook):\n",
    "\n",
    "* [MinMaxScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "* [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "When scaling data for machine learning, it's crucial to apply the transformation to the train and test sets carefully to avoid introducing data leakage. The key principle is that the test set should represent unseen data, simulating how the model will perform in real-world scenarios. In real-world scenarios we do not know the statistics of the data, thus we have to assume that the distribution of the training data is representatitve enough to reflect any new unseen data.\n",
    "\n",
    ">__Therefore, in most cases, the scaling parameters (e.g., mean and standard deviation for StandardScaler, or min and max for MinMaxScaler) must be computed from the training set and then applied to both the training and test sets. If you scale the data before splitting (i.e. the entire dataset together), this can lead to data leakage, where information from the test set influences the model and results in overly optimistic performance metrics.__\n",
    "\n",
    "Fortunately, the MinMaxScaler() and StandardScaler() handle most of this for us.\n",
    "\n",
    "_Note 1_: applying a scaler to a pandas.DataFrame will return a numpy.array object. In case you want a pandas.DataFrame object again, you have to convert the output back to a pandas.DataFrame. This however not really necessary. Either way, the code snippets below show how you can obtain a pandas.DataFrame again after applying the scaler.\n",
    "\n",
    "_Note 2_: The code snippets below do not check whether or not it makes sense to use Min-Max or Standard Scaling on the whole dataset. They just show how you can use the scalers to scale your data.\n",
    "\n",
    "### 11.1. Min-Max Scaling\n",
    "\n",
    "MinMax scaling is a normalization technique that transforms features to a specific range, typically between 0 and 1. It rescales each feature based on the formula:\n",
    "\n",
    "$$X_{min-max} = {X - X_{min} \\over X_{max} - X_{min}}$$\n",
    "\n",
    "where $X_{min}$ and $X_{max}$ are the minimum and maximum values of the feature, respectively. This ensures all features have the same scale, preserving the relationships between values while eliminating distortions caused by differing ranges.\n",
    "\n",
    "#### 11.1.1 When to use MinMax scaling:\n",
    "* For distance-based algorithms like K-Nearest Neighbors (KNN) or Neural Networks.\n",
    "* When features are not normally distributed.\n",
    "* In scenarios where feature ranges are bounded, such as pixel intensities in image processing.\n",
    "\n",
    "#### 11.1.2 Downsides of MinMax scaling:\n",
    "* Sensitive to outliers, as they can significantly affect the scaling range.\n",
    "* Does not change the shape of the data distribution, which may be a drawback for features with skewed distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing scaler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining min max scaler and its range\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# fit the scaler to train set and scale the train data (this will return a numpy.array)\n",
    "X_train_titanic_minmax = min_max_scaler.fit_transform(X_train_titanic)\n",
    "\n",
    "# scale the test data using the scaler that was fitted on the training data\n",
    "X_test_titanic_minmax = min_max_scaler.transform(X_test_titanic)\n",
    "\n",
    "# print the first 5 rows of the numpy.array containing the training data\n",
    "print(\"The first 5 rows of the X_train_bc_minmax numpy.array are: \\n{}\".format((X_train_titanic_minmax[:5, :])))\n",
    "\n",
    "# converting the training and test data back to a pandas.DataFrame\n",
    "df_X_train_titanic_minmax = pd.DataFrame(X_train_titanic_minmax, columns=X_train_titanic.columns)\n",
    "df_X_test_titanic_minmax = pd.DataFrame(X_test_titanic_minmax, columns=X_test_titanic.columns)\n",
    "\n",
    "# print the first 5 rows of the DataFrame containing the training data\n",
    "df_X_train_titanic_minmax.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: As you can see in the output above, applying the MinMaxScaler to the entire DataFrame does not make a lot of sense as the ordinal information of the 'Pclass' column gets lost. Instead it would make more sense to apply the MinMaxScaler only the columns where scaling is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2. Standard Scaling\n",
    "\n",
    "Standard scaling is a technique that centers features by subtracting the mean and scales them by dividing by the standard deviation. The formula is:\n",
    "\n",
    "$$X_{standard} = {X - \\mu \\over \\sigma}$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation of the feature. This process results in a distribution with a mean of 0 and a standard deviation of 1, making the features comparable in terms of variance\n",
    "\n",
    "#### 11.2.1 When to use Standard scaling:\n",
    "* For algorithms that assume features are normally distributed, such as Linear Regression, Logistic Regression, and Support Vector Machines (SVM).\n",
    "* When features have different units or large variance differences.\n",
    "* In cases where preserving outlier relationships is important, as standard scaling does not compress the range like MinMax scaling.\n",
    "\n",
    "#### 11.2.2 Downsides of Standard scaling:\n",
    "* May not perform well if the data is not normally distributed.\n",
    "* Outliers can still affect the scaling, as they contribute to the calculation of the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to train set and scale the train data (this will return a numpy.array)\n",
    "X_train_titanic_std = std_scaler.fit_transform(X_train_titanic)\n",
    "\n",
    "# scale the test data using the scaler that was fitted on the training data\n",
    "X_test_titanic_std = std_scaler.transform(X_test_titanic)\n",
    "\n",
    "# print the first 5 rows of the numpy.array containing the training data\n",
    "print(\"The first 5 rows of the X_test_titanic_std numpy.array are: \\n{}\".format((X_train_titanic_std[:5, :])))\n",
    "\n",
    "# converting the training and test data back to a pandas.DataFrame\n",
    "df_X_train_titanic_std = pd.DataFrame(X_train_titanic_std, columns=X_train_titanic.columns)\n",
    "df_X_test_titanic_std = pd.DataFrame(X_test_titanic_std, columns=X_test_titanic.columns)\n",
    "\n",
    "# print the first 5 rows of the DataFrame containing the training data\n",
    "df_X_train_titanic_std.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: As you can see in the output above, applying the StandardScaler to the entire DataFrame does not make a lot of sense as the encodings we generated before for the 'Sex' and 'Embarked' column are lost, as well as the ordinal information contained in 'Pclass'. Instead it would make more sense to apply the StandardScaler only the columns where scaling is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3. Exercise\n",
    "\n",
    "__Exercise 1__: Although the Material dataset is already normalized between 1 and 10, apply Min-Max scaling to the data.\n",
    "\n",
    "_Note_: you don't need to transform the output back to a pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee3b8b0f13ce1b575702cf6f1b3dd3d8df18dc5f202e0bca1f4b2f664d388be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
